{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Loading and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "raw_training_data = pd.read_pickle(r\"data\\training_data\\training_data_v1.0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before dropping missing values:  50644\n",
      "Number of rows after dropping missing values:  48048\n"
     ]
    }
   ],
   "source": [
    "#eliminate rows woth missing values in the policies\n",
    "print(\"Number of rows before dropping missing values: \", len(raw_training_data))\n",
    "nona_training_data = raw_training_data.dropna(subset=['C', 'E', 'G', 'S'])\n",
    "print(\"Number of rows after dropping missing values: \", len(nona_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique countries in the dataset\n",
    "countries = nona_training_data.location.unique()\n",
    "#Define the feature columns\n",
    "feat_columns = ['C', 'E', 'G', 'S']\n",
    "#Empty df to append the cleaned data country by country\n",
    "nozero_training_data = pd.DataFrame()\n",
    "\n",
    "# Here we remove the first rows for each country where all the features are zero\n",
    "# This is done to remove the initial period where the policies were not yet implemented\n",
    "for country in countries:\n",
    "    temp = nona_training_data.loc[nona_training_data['location'] == country]\n",
    "    temp = temp.reset_index(drop=True)\n",
    "    first_non_zero_index = temp.loc[(temp[feat_columns] != 0).any(axis=1)].index[0]\n",
    "    filtered_data = temp.iloc[first_non_zero_index:]\n",
    "    nozero_training_data = pd.concat([nozero_training_data, filtered_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for the model\n",
    "feat_policy_columns = ['C', 'E', 'G', 'S']\n",
    "feat_totals = ['total_cases', 'total_deaths']\n",
    "location = ['location']\n",
    "\n",
    "target_cases = ['new_cases']\n",
    "target_deaths = ['new_deaths']\n",
    "\n",
    "model_data = nozero_training_data[location + feat_policy_columns + feat_totals + target_cases + target_deaths].copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "model_data[feat_policy_columns] = scaler.fit_transform(model_data[feat_policy_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'location'  # Replace with the desired column name\n",
    "value_counts = nona_training_data[column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences per country\n",
    "def create_sequences_per_country(df, countries, feature_columns, seq_length):\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    for country in countries:\n",
    "        country_data = df[df[\"location\"] == country].drop(columns=[\"location\"]).reset_index(drop=True)\n",
    "        features = country_data[feature_columns].values\n",
    "        target = country_data[\"new_cases\"].values\n",
    "        \n",
    "        X, y = [], []\n",
    "        for i in range(len(features) - seq_length):\n",
    "            X.append(features[i : i + seq_length])\n",
    "            y.append(target[i + seq_length])\n",
    "        \n",
    "        X_list.append(np.array(X))\n",
    "        y_list.append(np.array(y))\n",
    "    \n",
    "    return np.vstack(X_list), np.hstack(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVIDTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, countries, feature_columns, seq_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): The full dataset containing all countries' data.\n",
    "            countries (list): List of country names to include.\n",
    "            feature_columns (list): List of feature column names.\n",
    "            seq_length (int): Number of time steps per sequence.\n",
    "        \"\"\"\n",
    "        # Generate sequences using the provided function\n",
    "        X, y = create_sequences_per_country(df, countries, feature_columns, seq_length)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Shape: (num_samples, seq_length, num_features)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)  # Shape: (num_samples, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return one sample (X sequence and corresponding y)\"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "    def get_number_features(self):\n",
    "        \"\"\"Return the number of features\"\"\"\n",
    "        return self.X.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(dataset, train_split=0.8, batch_size=32):\n",
    "    \"\"\"Creates sequential DataLoaders for training and testing\"\"\"\n",
    "    # countries = df[\"location\"].unique().tolist()  # Get all unique countries\n",
    "\n",
    "    # Create dataset\n",
    "    # dataset = COVIDTimeSeriesDataset(df, countries, feature_columns, seq_length)\n",
    "\n",
    "    # Split into train/test using time order\n",
    "    train_size = int(len(dataset) * train_split)\n",
    "    # Ensure that the split is sequential\n",
    "    train_dataset = torch.utils.data.Subset(dataset, range(train_size))  # First 80%\n",
    "    test_dataset = torch.utils.data.Subset(dataset, range(train_size, len(dataset)))  # Last 20%\n",
    "\n",
    "    # DataLoader (no shuffle for sequential ordering)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset = COVIDTimeSeriesDataset(df=model_data, countries=countries, \n",
    "                                 feature_columns=feat_policy_columns+feat_totals, seq_length=14)\n",
    "train_loader, test_loader = create_dataloaders(dataset=dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "input_size = dataset.get_number_features()  # Number of features\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, num_layers)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # RNN output\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step's output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2960119169.9462\n",
      "Epoch [2/20], Loss: 2959835260.3413\n",
      "Epoch [3/20], Loss: 2959552808.9649\n",
      "Epoch [4/20], Loss: 2959271799.2958\n",
      "Epoch [5/20], Loss: 2958992614.1894\n",
      "Epoch [6/20], Loss: 2958714634.4956\n",
      "Epoch [7/20], Loss: 2958439394.7562\n",
      "Epoch [8/20], Loss: 2958166565.3967\n",
      "Epoch [9/20], Loss: 2957908527.3801\n",
      "Epoch [10/20], Loss: 2957638625.3972\n",
      "Epoch [11/20], Loss: 2957364341.9657\n",
      "Epoch [12/20], Loss: 2957092844.8316\n",
      "Epoch [13/20], Loss: 2956828194.1578\n",
      "Epoch [14/20], Loss: 2956567616.9306\n",
      "Epoch [15/20], Loss: 2956916606.9582\n",
      "Epoch [16/20], Loss: 2956124266.4580\n",
      "Epoch [17/20], Loss: 2956024279.1218\n",
      "Epoch [18/20], Loss: 2955591032.9083\n",
      "Epoch [19/20], Loss: 2955802588.3032\n",
      "Epoch [20/20], Loss: 2956741175.9724\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "input_size = dataset.get_number_features()  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1  # Predicting one value (e.g., next time step)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.MSELoss()  # For regression tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        y_pred = model(X_batch)  # Forward pass\n",
    "        loss = criterion(y_pred, y_batch)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46891, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46275, 14, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
